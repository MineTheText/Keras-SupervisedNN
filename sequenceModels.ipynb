{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(438, 7)\n"
     ]
    }
   ],
   "source": [
    "import sqlite3 as sql\n",
    "import numpy as np \n",
    "from sklearn.preprocessing import normalize\n",
    "crDatabase = sql.connect('customerReviews.db')\n",
    "cursorReviews = crDatabase.execute(\"select a.closenessCentrality,a.betweennessCentrality,a.degreeCentrality, b.avgRating, b.totalReviews, b.price, b.unlockedCellRank from productsCentrality a inner join productRanks b on a.product = b.product and a.reviewsDate = b.reviewsDate and a.product in ('B01D0JV7AO','B07GS1V652','B01F482BTK','B01CR1FQMG','B0786PRKBP','B0788ZD69Z') and a.reviewsDate not in ('052319', '053019', '060319', '062019', '062119') order by a.reviewsDate;\")\n",
    "rows=cursorReviews.fetchall()\n",
    "dataSamples = np.array(rows)\n",
    "\n",
    "dataSamples1Thru4 = dataSamples[:, :4]\n",
    "dataSamples5 = dataSamples[:, 4:5]\n",
    "dataSamples6 = dataSamples[:, 5:6]\n",
    "dataSamples7 = dataSamples[:, 6:7]\n",
    "\n",
    "# The below code normalizes the numpy data \n",
    "mean = dataSamples5[:].mean(axis=0)\n",
    "dataSamples5 -= mean \n",
    "std = dataSamples5[:].std(axis=0)\n",
    "dataSamples5 /= std\n",
    "mean = dataSamples6[:].mean(axis=0)\n",
    "dataSamples6 -= mean \n",
    "std = dataSamples6[:].std(axis=0)\n",
    "dataSamples6 /= std\n",
    "float_data = np.concatenate((dataSamples1Thru4, dataSamples5,dataSamples6,dataSamples7), axis=1)\n",
    "# updated_float_data = series_to_supervised(float_data)\n",
    "print(float_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the pollution for the next hour based on the past sequence data\n",
    "# Based on the LSTM supervised model \n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate LSTM Forecast Model - Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.8081118e-02  8.3840408e+00  1.6318848e+01 ... -2.7536867e+00\n",
      "  -2.5457678e+00  1.4960000e+03]\n",
      " [ 8.1434108e-02  0.0000000e+00  1.1427933e+01 ... -1.4286635e+00\n",
      "  -2.4336376e+00  5.1920000e+03]\n",
      " [ 1.1330588e-01  1.8695000e+02  1.7356615e+01 ... -6.0070920e+00\n",
      "  -2.9305222e+00  7.5680000e+03]\n",
      " ...\n",
      " [ 8.5253775e-02  3.3994231e+00  1.5900667e+01 ... -1.3321662e-01\n",
      "  -3.7586632e+00  3.3440000e+03]\n",
      " [ 1.1999633e-01  2.8790861e+04  2.0367315e+01 ... -5.5575309e+00\n",
      "  -2.9305222e+00  3.4320000e+03]\n",
      " [ 1.1999633e-01  4.8681271e-01  2.9653547e+00 ...  5.7761502e+00\n",
      "   5.5165157e+00  4.4000000e+02]]\n",
      "['var1(t-1)' 'var2(t-1)' 'var3(t-1)' 'var4(t-1)' 'var5(t-1)' 'var6(t-1)'\n",
      " 'var7(t-1)' 'var1(t)' 'var2(t)' 'var3(t)' 'var4(t)' 'var5(t)' 'var6(t)'\n",
      " 'var7(t)']\n",
      "['var1(t-1)' 'var2(t-1)' 'var3(t-1)' 'var4(t-1)' 'var5(t-1)' 'var6(t-1)'\n",
      " 'var7(t-1)' 'var7(t)']\n",
      "   var1(t-1)  var2(t-1)  var3(t-1)  var4(t-1)  var5(t-1)  var6(t-1)  \\\n",
      "1   0.219660   0.000032   0.307614      0.250   0.231970   0.133846   \n",
      "2   0.318635   0.000000   0.215419      0.375   0.326445   0.145892   \n",
      "3   0.554877   0.000706   0.327176      0.125   0.000000   0.092511   \n",
      "4   0.171143   0.000000   0.042930      0.875   0.301139   1.000000   \n",
      "5   0.554877   0.000000   0.240991      0.875   0.190215   0.288244   \n",
      "\n",
      "   var7(t-1)   var7(t)  \n",
      "1   0.193182  0.670455  \n",
      "2   0.670455  0.977273  \n",
      "3   0.977273  0.181818  \n",
      "4   0.181818  0.011364  \n",
      "5   0.011364  0.613636  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "values = float_data\n",
    "# integer encode direction\n",
    "encoder = LabelEncoder()\n",
    "values[:,6] = encoder.fit_transform(values[:,6])\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "scaled_reverse = scaler.inverse_transform(values)\n",
    "print(scaled_reverse)\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, 1, 1)\n",
    "print(reframed.columns.values)\n",
    "# drop columns we don't want to predict\n",
    "reframed.drop(['var1(t)', 'var2(t)', 'var3(t)', 'var4(t)', 'var5(t)', 'var6(t)'], axis=1, inplace=True)\n",
    "print(reframed.columns.values)\n",
    "print(reframed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM expects the data in the format [samples, timesteps, features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 1, 7) (300,) (137, 1, 7) (137,)\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "values = reframed.values\n",
    "n_days = 73 * 6\n",
    "n_train_days = 50 * 6\n",
    "train = values[:n_train_days, :]\n",
    "test = values[n_train_days:n_days, :]\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\vmt907\\documents\\mynncode\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\vmt907\\documents\\mynncode\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\vmt907\\documents\\mynncode\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\vmt907\\documents\\mynncode\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\vmt907\\documents\\mynncode\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\vmt907\\documents\\mynncode\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\vmt907\\documents\\mynncode\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\vmt907\\documents\\mynncode\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\vmt907\\documents\\mynncode\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\vmt907\\documents\\mynncode\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\vmt907\\documents\\mynncode\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\vmt907\\documents\\mynncode\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\vmt907\\documents\\mynncode\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\vmt907\\documents\\mynncode\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\vmt907\\documents\\mynncode\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\vmt907\\documents\\mynncode\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\vmt907\\documents\\mynncode\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From c:\\users\\vmt907\\documents\\mynncode\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\vmt907\\documents\\mynncode\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 300 samples, validate on 137 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 0.3612 - val_loss: 0.2809\n",
      "Epoch 2/50\n",
      " - 0s - loss: 0.3263 - val_loss: 0.2542\n",
      "Epoch 3/50\n",
      " - 0s - loss: 0.3006 - val_loss: 0.2360\n",
      "Epoch 4/50\n",
      " - 0s - loss: 0.2797 - val_loss: 0.2239\n",
      "Epoch 5/50\n",
      " - 0s - loss: 0.2628 - val_loss: 0.2167\n",
      "Epoch 6/50\n",
      " - 0s - loss: 0.2489 - val_loss: 0.2112\n",
      "Epoch 7/50\n",
      " - 0s - loss: 0.2382 - val_loss: 0.2085\n",
      "Epoch 8/50\n",
      " - 0s - loss: 0.2310 - val_loss: 0.2089\n",
      "Epoch 9/50\n",
      " - 0s - loss: 0.2259 - val_loss: 0.2114\n",
      "Epoch 10/50\n",
      " - 0s - loss: 0.2225 - val_loss: 0.2142\n",
      "Epoch 11/50\n",
      " - 0s - loss: 0.2205 - val_loss: 0.2166\n",
      "Epoch 12/50\n",
      " - 0s - loss: 0.2191 - val_loss: 0.2188\n",
      "Epoch 13/50\n",
      " - 0s - loss: 0.2180 - val_loss: 0.2208\n",
      "Epoch 14/50\n",
      " - 0s - loss: 0.2170 - val_loss: 0.2221\n",
      "Epoch 15/50\n",
      " - 0s - loss: 0.2163 - val_loss: 0.2228\n",
      "Epoch 16/50\n",
      " - 0s - loss: 0.2158 - val_loss: 0.2231\n",
      "Epoch 17/50\n",
      " - 0s - loss: 0.2153 - val_loss: 0.2232\n",
      "Epoch 18/50\n",
      " - 0s - loss: 0.2149 - val_loss: 0.2232\n",
      "Epoch 19/50\n",
      " - 0s - loss: 0.2145 - val_loss: 0.2233\n",
      "Epoch 20/50\n",
      " - 0s - loss: 0.2141 - val_loss: 0.2233\n",
      "Epoch 21/50\n",
      " - 0s - loss: 0.2138 - val_loss: 0.2233\n",
      "Epoch 22/50\n",
      " - 0s - loss: 0.2134 - val_loss: 0.2234\n",
      "Epoch 23/50\n",
      " - 0s - loss: 0.2131 - val_loss: 0.2236\n",
      "Epoch 24/50\n",
      " - 0s - loss: 0.2127 - val_loss: 0.2238\n",
      "Epoch 25/50\n",
      " - 0s - loss: 0.2124 - val_loss: 0.2240\n",
      "Epoch 26/50\n",
      " - 0s - loss: 0.2120 - val_loss: 0.2240\n",
      "Epoch 27/50\n",
      " - 0s - loss: 0.2117 - val_loss: 0.2240\n",
      "Epoch 28/50\n",
      " - 0s - loss: 0.2114 - val_loss: 0.2241\n",
      "Epoch 29/50\n",
      " - 0s - loss: 0.2111 - val_loss: 0.2242\n",
      "Epoch 30/50\n",
      " - 0s - loss: 0.2108 - val_loss: 0.2246\n",
      "Epoch 31/50\n",
      " - 0s - loss: 0.2105 - val_loss: 0.2250\n",
      "Epoch 32/50\n",
      " - 0s - loss: 0.2102 - val_loss: 0.2257\n",
      "Epoch 33/50\n",
      " - 0s - loss: 0.2098 - val_loss: 0.2264\n",
      "Epoch 34/50\n",
      " - 0s - loss: 0.2094 - val_loss: 0.2273\n",
      "Epoch 35/50\n",
      " - 0s - loss: 0.2090 - val_loss: 0.2283\n",
      "Epoch 36/50\n",
      " - 0s - loss: 0.2085 - val_loss: 0.2292\n",
      "Epoch 37/50\n",
      " - 0s - loss: 0.2081 - val_loss: 0.2299\n",
      "Epoch 38/50\n",
      " - 0s - loss: 0.2077 - val_loss: 0.2305\n",
      "Epoch 39/50\n",
      " - 0s - loss: 0.2074 - val_loss: 0.2310\n",
      "Epoch 40/50\n",
      " - 0s - loss: 0.2071 - val_loss: 0.2316\n",
      "Epoch 41/50\n",
      " - 0s - loss: 0.2069 - val_loss: 0.2321\n",
      "Epoch 42/50\n",
      " - 0s - loss: 0.2066 - val_loss: 0.2326\n",
      "Epoch 43/50\n",
      " - 0s - loss: 0.2063 - val_loss: 0.2330\n",
      "Epoch 44/50\n",
      " - 0s - loss: 0.2061 - val_loss: 0.2332\n",
      "Epoch 45/50\n",
      " - 0s - loss: 0.2059 - val_loss: 0.2334\n",
      "Epoch 46/50\n",
      " - 0s - loss: 0.2057 - val_loss: 0.2334\n",
      "Epoch 47/50\n",
      " - 0s - loss: 0.2055 - val_loss: 0.2335\n",
      "Epoch 48/50\n",
      " - 0s - loss: 0.2053 - val_loss: 0.2337\n",
      "Epoch 49/50\n",
      " - 0s - loss: 0.2051 - val_loss: 0.2341\n",
      "Epoch 50/50\n",
      " - 0s - loss: 0.2049 - val_loss: 0.2344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from matplotlib import pyplot\n",
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.104\n"
     ]
    }
   ],
   "source": [
    "from numpy import concatenate \n",
    "from pandas import concat\n",
    "from math import sqrt \n",
    "from sklearn.metrics import mean_squared_error\n",
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "# calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 1, 7) (137, 7) (300,) (137, 1) (437, 8) (137, 1)\n",
      "(300, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[34.317646 ],\n",
       "        [16.946754 ],\n",
       "        [32.35408  ],\n",
       "        [28.82535  ],\n",
       "        [36.060997 ],\n",
       "        [23.003864 ],\n",
       "        [33.614304 ],\n",
       "        [30.780886 ],\n",
       "        [34.080875 ],\n",
       "        [34.622265 ],\n",
       "        [15.313161 ],\n",
       "        [20.754116 ],\n",
       "        [17.389261 ],\n",
       "        [37.96326  ],\n",
       "        [31.418362 ],\n",
       "        [34.01576  ],\n",
       "        [30.949524 ],\n",
       "        [19.619938 ],\n",
       "        [33.780823 ],\n",
       "        [29.13812  ],\n",
       "        [33.190388 ],\n",
       "        [33.526047 ],\n",
       "        [15.883192 ],\n",
       "        [20.038845 ],\n",
       "        [34.93606  ],\n",
       "        [20.524055 ],\n",
       "        [45.041195 ],\n",
       "        [41.824085 ],\n",
       "        [36.489407 ],\n",
       "        [26.509174 ],\n",
       "        [23.063028 ],\n",
       "        [35.902073 ],\n",
       "        [44.86083  ],\n",
       "        [31.832466 ],\n",
       "        [25.104837 ],\n",
       "        [41.063908 ],\n",
       "        [35.217354 ],\n",
       "        [39.982944 ],\n",
       "        [23.900307 ],\n",
       "        [26.819487 ],\n",
       "        [31.344608 ],\n",
       "        [36.482155 ],\n",
       "        [40.5079   ],\n",
       "        [40.664516 ],\n",
       "        [41.762367 ],\n",
       "        [34.525246 ],\n",
       "        [26.23136  ],\n",
       "        [22.783798 ],\n",
       "        [40.182407 ],\n",
       "        [37.57477  ],\n",
       "        [29.165552 ],\n",
       "        [21.287176 ],\n",
       "        [24.243282 ],\n",
       "        [42.206825 ],\n",
       "        [40.67341  ],\n",
       "        [39.72859  ],\n",
       "        [33.02563  ],\n",
       "        [25.318056 ],\n",
       "        [22.833136 ],\n",
       "        [39.23885  ],\n",
       "        [36.998417 ],\n",
       "        [35.27672  ],\n",
       "        [23.58665  ],\n",
       "        [32.307796 ],\n",
       "        [17.896112 ],\n",
       "        [19.728792 ],\n",
       "        [19.128578 ],\n",
       "        [37.678635 ],\n",
       "        [22.02915  ],\n",
       "        [37.60991  ],\n",
       "        [35.783657 ],\n",
       "        [18.190767 ],\n",
       "        [47.04336  ],\n",
       "        [44.36175  ],\n",
       "        [27.219257 ],\n",
       "        [25.300268 ],\n",
       "        [42.154114 ],\n",
       "        [44.79778  ],\n",
       "        [46.140472 ],\n",
       "        [42.686123 ],\n",
       "        [34.49107  ],\n",
       "        [22.926804 ],\n",
       "        [21.316896 ],\n",
       "        [40.50065  ],\n",
       "        [48.786743 ],\n",
       "        [46.04403  ],\n",
       "        [24.133936 ],\n",
       "        [25.016466 ],\n",
       "        [37.603634 ],\n",
       "        [40.801563 ],\n",
       "        [55.24252  ],\n",
       "        [48.078808 ],\n",
       "        [43.77626  ],\n",
       "        [26.882872 ],\n",
       "        [43.672657 ],\n",
       "        [25.982817 ],\n",
       "        [52.1696   ],\n",
       "        [23.00648  ],\n",
       "        [40.560417 ],\n",
       "        [46.448734 ],\n",
       "        [26.872158 ],\n",
       "        [42.91204  ],\n",
       "        [45.913403 ],\n",
       "        [55.24491  ],\n",
       "        [47.431976 ],\n",
       "        [41.11215  ],\n",
       "        [27.952936 ],\n",
       "        [24.308012 ],\n",
       "        [54.12354  ],\n",
       "        [51.978584 ],\n",
       "        [48.82606  ],\n",
       "        [27.176235 ],\n",
       "        [23.873938 ],\n",
       "        [43.390045 ],\n",
       "        [49.35427  ],\n",
       "        [50.9948   ],\n",
       "        [49.268078 ],\n",
       "        [26.150743 ],\n",
       "        [22.642027 ],\n",
       "        [39.567318 ],\n",
       "        [51.553257 ],\n",
       "        [50.393387 ],\n",
       "        [44.526066 ],\n",
       "        [29.417618 ],\n",
       "        [ 4.5091634],\n",
       "        [51.68155  ],\n",
       "        [43.926746 ],\n",
       "        [52.145073 ],\n",
       "        [50.785873 ],\n",
       "        [34.291756 ],\n",
       "        [27.63481  ],\n",
       "        [20.954302 ],\n",
       "        [45.81224  ],\n",
       "        [46.059906 ],\n",
       "        [33.06606  ],\n",
       "        [26.779871 ],\n",
       "        [22.289139 ]], dtype=float32), array([[21.],\n",
       "        [47.],\n",
       "        [52.],\n",
       "        [40.],\n",
       "        [ 6.],\n",
       "        [49.],\n",
       "        [20.],\n",
       "        [53.],\n",
       "        [76.],\n",
       "        [35.],\n",
       "        [66.],\n",
       "        [67.],\n",
       "        [62.],\n",
       "        [25.],\n",
       "        [60.],\n",
       "        [76.],\n",
       "        [32.],\n",
       "        [71.],\n",
       "        [25.],\n",
       "        [60.],\n",
       "        [76.],\n",
       "        [32.],\n",
       "        [62.],\n",
       "        [71.],\n",
       "        [18.],\n",
       "        [36.],\n",
       "        [ 3.],\n",
       "        [33.],\n",
       "        [ 8.],\n",
       "        [21.],\n",
       "        [35.],\n",
       "        [30.],\n",
       "        [ 8.],\n",
       "        [15.],\n",
       "        [46.],\n",
       "        [47.],\n",
       "        [33.],\n",
       "        [48.],\n",
       "        [38.],\n",
       "        [27.],\n",
       "        [12.],\n",
       "        [ 5.],\n",
       "        [ 3.],\n",
       "        [21.],\n",
       "        [50.],\n",
       "        [18.],\n",
       "        [29.],\n",
       "        [42.],\n",
       "        [16.],\n",
       "        [60.],\n",
       "        [19.],\n",
       "        [28.],\n",
       "        [25.],\n",
       "        [ 2.],\n",
       "        [ 0.],\n",
       "        [18.],\n",
       "        [20.],\n",
       "        [37.],\n",
       "        [22.],\n",
       "        [48.],\n",
       "        [ 2.],\n",
       "        [17.],\n",
       "        [21.],\n",
       "        [50.],\n",
       "        [25.],\n",
       "        [41.],\n",
       "        [21.],\n",
       "        [ 1.],\n",
       "        [24.],\n",
       "        [15.],\n",
       "        [31.],\n",
       "        [43.],\n",
       "        [23.],\n",
       "        [46.],\n",
       "        [43.],\n",
       "        [41.],\n",
       "        [19.],\n",
       "        [ 2.],\n",
       "        [ 6.],\n",
       "        [ 3.],\n",
       "        [ 7.],\n",
       "        [45.],\n",
       "        [51.],\n",
       "        [42.],\n",
       "        [ 3.],\n",
       "        [16.],\n",
       "        [44.],\n",
       "        [46.],\n",
       "        [ 9.],\n",
       "        [ 7.],\n",
       "        [ 6.],\n",
       "        [13.],\n",
       "        [ 7.],\n",
       "        [46.],\n",
       "        [ 5.],\n",
       "        [33.],\n",
       "        [ 6.],\n",
       "        [55.],\n",
       "        [ 5.],\n",
       "        [24.],\n",
       "        [48.],\n",
       "        [12.],\n",
       "        [ 7.],\n",
       "        [ 4.],\n",
       "        [30.],\n",
       "        [10.],\n",
       "        [29.],\n",
       "        [52.],\n",
       "        [ 4.],\n",
       "        [ 7.],\n",
       "        [38.],\n",
       "        [40.],\n",
       "        [57.],\n",
       "        [10.],\n",
       "        [10.],\n",
       "        [ 7.],\n",
       "        [27.],\n",
       "        [40.],\n",
       "        [62.],\n",
       "        [11.],\n",
       "        [ 3.],\n",
       "        [16.],\n",
       "        [11.],\n",
       "        [31.],\n",
       "        [41.],\n",
       "        [ 4.],\n",
       "        [ 5.],\n",
       "        [ 4.],\n",
       "        [17.],\n",
       "        [ 6.],\n",
       "        [42.],\n",
       "        [53.],\n",
       "        [ 9.],\n",
       "        [22.],\n",
       "        [ 7.],\n",
       "        [38.],\n",
       "        [39.],\n",
       "        [ 5.]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_X.shape, test_X.shape, train_y.shape, test_y.shape, reframed.shape, yhat.shape)\n",
    "train_tranformed_X = train_X.reshape((train_X.shape[0], train_X.shape[2]))\n",
    "sevencols_dataset = concatenate((train_tranformed_X, test_X), axis=0)\n",
    "train_tranformed_y = np.reshape(train_y, (-1, 1))\n",
    "print(train_tranformed_y.shape)\n",
    "singlecol_dataset = concatenate((train_tranformed_y, yhat), axis=0)\n",
    "total_dataset = concatenate((sevencols_dataset[:,:6], singlecol_dataset), axis=1)\n",
    "total_dataset_inverse = scaler.inverse_transform(total_dataset[:,:7])\n",
    "total_dataset_inverse[300:,6:7], float_data[300:,6:7]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
